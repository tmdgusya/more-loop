#!/usr/bin/env bash
set -euo pipefail

# more-loop — Iterative development script wrapping the claude CLI

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
BOLD='\033[1m'
NC='\033[0m'

# Defaults
MAX_ITERATIONS=5
MAX_TASKS=""
MODEL="opus"
VERBOSE=false
PROMPT_FILE=""
VERIFY_FILE=""
RESUME_DIR=""
WEB_MODE=false
APPROVE_MODE=false
APPROVE_EVERY=false
APPROVE_TIMEOUT=180
ORACLE_MODE=false
WEB_PORT=""

# Shared data directory for web dashboard files
DATA_DIR="${XDG_DATA_HOME:-${HOME}/.local/share}/more-loop"

# Script directory for finding system-prompts in dev mode
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

load_system_prompt() {
  local name="$1"
  # Dev mode: repo files first
  if [[ -f "${SCRIPT_DIR}/system-prompts/${name}.md" ]]; then
    cat "${SCRIPT_DIR}/system-prompts/${name}.md"
    return
  fi
  # Install mode: DATA_DIR
  if [[ -f "${DATA_DIR}/system-prompts/${name}.md" ]]; then
    cat "${DATA_DIR}/system-prompts/${name}.md"
    return
  fi
  echo ""
}

# State variables for web dashboard
RUN_NAME=""
CURRENT_ITERATION=0
PHASE=""

usage() {
  cat <<'EOF'
Usage: more-loop [OPTIONS] <prompt-file> [verify-file]
       more-loop --resume <run-dir> [OPTIONS]

Arguments:
  prompt-file             Spec/prompt describing what to build
  verify-file             Verification plan (shell script or markdown)
                          If shell script (.sh): runs it, exit 0 = pass
                          If markdown (.md): claude evaluates checklist
                          If omitted: skips verification step

Options:
  -n, --iterations N      Max iterations (default: 5)
  -m, --model MODEL       Model to use (default: opus)
  -v, --verbose           Show full claude output
  -w, --web               Start web dashboard server
  -a, --approve           Enable approval mode (pause after bootstrap only)
  --approve-every         Pause after EVERY iteration (not just bootstrap)
  --approve-timeout N     Approval timeout in seconds (default: 180, 0 = infinite)
  --oracle                Enable Oracle Test-First Architect phase before iterations
  --port PORT             Web server port (default: auto-select)
  --max-tasks N           Max tasks in bootstrap (default: same as iterations, clamped to <= iterations)
  --resume DIR            Resume an interrupted run from its run directory
  -h, --help              Show help
EOF
}

log() {
  echo -e "${BLUE}${BOLD}$1${NC}" >&2
}

log_pass() {
  echo -e "${GREEN}$1${NC}" >&2
}

log_fail() {
  echo -e "${RED}$1${NC}" >&2
}

log_warn() {
  echo -e "${YELLOW}$1${NC}" >&2
}

# Write state.json for web dashboard consumption
# Uses python3 json module for reliable escaping (no bash string mangling)
write_state_json() {
  local phase="$1"
  local current_task="$2"

  # If current_task is empty, try to preserve from existing state.json
  if [[ -z "$current_task" ]] && [[ -f "${RUN_DIR}/state.json" ]]; then
    current_task="$(python3 -c "
import json, sys
try:
    d = json.load(open(sys.argv[1]))
    print(d.get('current_task_name', ''), end='')
except: pass
" "${RUN_DIR}/state.json" 2>/dev/null)" || true
  fi

  # Read started_at from existing state if available
  local started_at=""
  if [[ -f "${RUN_DIR}/state.json" ]]; then
    started_at="$(python3 -c "
import json, sys
try:
    d = json.load(open(sys.argv[1]))
    print(d.get('started_at', ''), end='')
except: pass
" "${RUN_DIR}/state.json" 2>/dev/null)" || true
  fi

  # Generate state.json with python3 for correct JSON escaping
  python3 -c "
import json, glob, os, re, sys
from datetime import datetime, timezone

run_dir = sys.argv[1]
phase = sys.argv[2]
current_task = sys.argv[3]
model = sys.argv[4]
max_iterations = int(sys.argv[5])
current_iteration = int(sys.argv[6])
started_at = sys.argv[7]
approve_timeout = int(sys.argv[8])

updated_at = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
if not started_at:
    started_at = updated_at

# Read files
tasks_content = ''
acceptance_content = ''
tasks_file = os.path.join(run_dir, 'tasks.md')
acceptance_file = os.path.join(run_dir, 'acceptance.md')
if os.path.isfile(tasks_file):
    tasks_content = open(tasks_file).read()
if os.path.isfile(acceptance_file):
    acceptance_content = open(acceptance_file).read()

# Count tasks
tasks_total = len(re.findall(r'^- \[.\]', tasks_content, re.MULTILINE))
tasks_completed = len(re.findall(r'^- \[x\]', tasks_content, re.MULTILINE))

# Build iterations array
iterations = []
iter_dir = os.path.join(run_dir, 'iterations')
if os.path.isdir(iter_dir):
    files = sorted(glob.glob(os.path.join(iter_dir, '*.md')))
    for f in files:
        name = os.path.basename(f)
        m = re.match(r'^(\d+)(-(verify|honesty))?\.md$', name)
        if not m:
            continue
        num = int(m.group(1))
        match_type = m.group(3)  # None, 'verify', or 'honesty'

        # Find or create entry for this iteration number
        entry = next((e for e in iterations if e['number'] == num), None)
        if entry is None:
            entry = {'number': num, 'summary': '', 'verify_result': '', 'verify_detail': '', 'honesty_result': '', 'honesty_detail': ''}
            iterations.append(entry)

        content = open(f).read()
        if match_type == 'honesty':
            first_line = content.split('\n', 1)[0].strip().upper()
            if first_line.startswith('HONEST'):
                entry['honesty_result'] = 'HONEST'
            elif first_line.startswith('DISHONEST'):
                entry['honesty_result'] = 'DISHONEST'
            else:
                entry['honesty_result'] = 'SKIP'
            entry['honesty_detail'] = content
        elif match_type == 'verify':
            first_line = content.split('\n', 1)[0].strip().upper()
            if first_line.startswith('PASS'):
                entry['verify_result'] = 'PASS'
            elif first_line.startswith('FAIL'):
                entry['verify_result'] = 'FAIL'
            else:
                entry['verify_result'] = 'SKIP'
            entry['verify_detail'] = content
        else:
            entry['summary'] = content

    iterations.sort(key=lambda e: e['number'])

state = {
    'run_name': os.path.basename(run_dir),
    'model': model,
    'max_iterations': max_iterations,
    'current_iteration': current_iteration,
    'phase': phase,
    'tasks_total': tasks_total,
    'tasks_completed': tasks_completed,
    'tasks': tasks_content,
    'acceptance': acceptance_content,
    'iterations': iterations,
    'current_task_name': current_task,
    'started_at': started_at,
    'updated_at': updated_at,
    'approve_timeout': approve_timeout,
}

# Atomic write: temp file + os.rename to prevent readers seeing partial JSON
import tempfile
target = os.path.join(run_dir, 'state.json')
fd, tmp = tempfile.mkstemp(dir=run_dir, suffix='.tmp')
try:
    os.write(fd, json.dumps(state, indent=2, ensure_ascii=False).encode())
    os.close(fd)
    os.rename(tmp, target)
except BaseException:
    os.close(fd)
    try: os.unlink(tmp)
    except OSError: pass
    raise
" "$RUN_DIR" "$phase" "$current_task" "$MODEL" "$MAX_ITERATIONS" "$CURRENT_ITERATION" "$started_at" "$APPROVE_TIMEOUT"
}

# Helper: write state only if web or approve mode is enabled
maybe_write_state() {
  local phase="$1"
  local current_task="${2:-}"
  if [[ "$WEB_MODE" == "true" ]] || [[ "$APPROVE_MODE" == "true" ]]; then
    write_state_json "$phase" "$current_task"
  fi
}

parse_args() {
  while [[ $# -gt 0 ]]; do
    case "$1" in
      -n|--iterations)
        MAX_ITERATIONS="$2"
        shift 2
        ;;
      -m|--model)
        MODEL="$2"
        shift 2
        ;;
      -v|--verbose)
        VERBOSE=true
        shift
        ;;
      -h|--help)
        usage
        exit 0
        ;;
      -w|--web)
        WEB_MODE=true
        shift
        ;;
      -a|--approve)
        APPROVE_MODE=true
        shift
        ;;
      --approve-every)
        APPROVE_MODE=true
        APPROVE_EVERY=true
        shift
        ;;
      --approve-timeout)
        APPROVE_TIMEOUT="$2"
        shift 2
        ;;
      --max-tasks)
        MAX_TASKS="$2"
        shift 2
        ;;
      --resume)
        RESUME_DIR="$2"
        shift 2
        ;;
      --port)
        WEB_PORT="$2"
        shift 2
        ;;
      --oracle)
        ORACLE_MODE=true
        shift
        ;;
      -*)
        echo "Unknown option: $1" >&2
        usage >&2
        exit 1
        ;;
      *)
        if [[ -z "$PROMPT_FILE" ]]; then
          PROMPT_FILE="$1"
        elif [[ -z "$VERIFY_FILE" ]]; then
          VERIFY_FILE="$1"
        else
          echo "Unexpected argument: $1" >&2
          usage >&2
          exit 1
        fi
        shift
        ;;
    esac
  done

  # --resume mode: validate run directory
  if [[ -n "$RESUME_DIR" ]]; then
    if [[ ! -d "$RESUME_DIR" ]]; then
      echo "Error: resume directory not found: $RESUME_DIR" >&2
      exit 1
    fi
    if [[ ! -f "$RESUME_DIR/tasks.md" || ! -f "$RESUME_DIR/acceptance.md" ]]; then
      echo "Error: resume directory missing tasks.md or acceptance.md: $RESUME_DIR" >&2
      exit 1
    fi
    # Recover PROMPT_FILE and VERIFY_FILE from the run directory
    if [[ -z "$PROMPT_FILE" && -f "$RESUME_DIR/prompt.md" ]]; then
      PROMPT_FILE="$RESUME_DIR/prompt.md"
    fi
    return
  fi

  if [[ -z "$PROMPT_FILE" ]]; then
    echo "Error: prompt-file is required" >&2
    usage >&2
    exit 1
  fi

  if [[ ! -f "$PROMPT_FILE" ]]; then
    echo "Error: prompt file not found: $PROMPT_FILE" >&2
    exit 1
  fi

  if [[ -n "$VERIFY_FILE" && ! -f "$VERIFY_FILE" ]]; then
    echo "Error: verify file not found: $VERIFY_FILE" >&2
    exit 1
  fi
}

setup_run_dir() {
  local base_name
  base_name="$(basename "$PROMPT_FILE")"
  base_name="${base_name%.*}"

  RUN_DIR=".more-loop/${base_name}"
  RUN_NAME="${base_name}"
  if [[ -d "$RUN_DIR" ]]; then
    RUN_DIR=".more-loop/${base_name}-$(date +%s)"
    RUN_NAME="${base_name}-$(date +%s)"
  fi

  mkdir -p "${RUN_DIR}/iterations"
  cp "$PROMPT_FILE" "${RUN_DIR}/prompt.md"

  if [[ -n "$VERIFY_FILE" ]]; then
    cp "$VERIFY_FILE" "${RUN_DIR}/$(basename "$VERIFY_FILE")"
  fi
}

resume_run_dir() {
  RUN_DIR="$RESUME_DIR"
  RUN_NAME="$(basename "$RUN_DIR")"

  # Find the last completed iteration number
  local last_iter=0
  for f in "${RUN_DIR}"/iterations/[0-9]*.md; do
    [[ -f "$f" ]] || continue
    local basename
    basename="$(basename "$f" .md)"
    # Skip verify files (e.g., 3-verify.md)
    [[ "$basename" =~ -verify$ ]] && continue
    local num="${basename}"
    if [[ "$num" =~ ^[0-9]+$ ]] && [[ "$num" -gt "$last_iter" ]]; then
      last_iter="$num"
    fi
  done

  RESUME_FROM=$((last_iter + 1))
}

run_claude() {
  local prompt="$1"
  local system_prompt="${2:-}"
  local sys_args=()
  [[ -n "$system_prompt" ]] && sys_args=(--append-system-prompt "$system_prompt")

  if [[ "$VERBOSE" == true ]]; then
    echo -e "${YELLOW}━━━ PROMPT ━━━${NC}" >&2
    echo "$prompt" | head -40 >&2
    local prompt_lines
    prompt_lines="$(echo "$prompt" | wc -l)"
    if [[ "$prompt_lines" -gt 40 ]]; then
      echo -e "${YELLOW}... (${prompt_lines} lines total, truncated)${NC}" >&2
    fi
    if [[ -n "$system_prompt" ]]; then
      echo -e "${YELLOW}━━━ SYSTEM PROMPT ━━━${NC}" >&2
      echo "$system_prompt" >&2
    fi
    echo -e "${YELLOW}━━━ CLAUDE OUTPUT ━━━${NC}" >&2
  fi

  local output
  local rc=0

  if [[ "$VERBOSE" == true ]]; then
    # Show claude stdout in real-time on stderr, while still capturing it
    # stderr from claude goes to terminal directly (fd 3 trick avoids merging into $output)
    exec 3>&2
    output="$(claude -p --model "$MODEL" --permission-mode bypassPermissions "${sys_args[@]}" "$prompt" 2>&3 | tee /dev/stderr)" || rc=$?
    exec 3>&-
    echo -e "${YELLOW}━━━ END ━━━${NC}" >&2
  else
    output="$(claude -p --model "$MODEL" --permission-mode bypassPermissions "${sys_args[@]}" "$prompt" 2>/dev/null)" || rc=$?
  fi

  if [[ $rc -ne 0 ]]; then
    log_fail "claude exited with code $rc"
    echo "$output"
    return $rc
  fi

  echo "$output"
}

bootstrap() {
  log "[0/${MAX_ITERATIONS}] Bootstrap — generating tasks and acceptance criteria"

  # Calculate max_tasks — must be <= iterations to be completable
  local max_tasks="${MAX_TASKS}"
  if [[ -z "$max_tasks" ]]; then
    max_tasks="$MAX_ITERATIONS"
  fi
  if [[ "$max_tasks" -gt "$MAX_ITERATIONS" ]]; then
    log_warn "max_tasks (${max_tasks}) > iterations (${MAX_ITERATIONS}), clamping to ${MAX_ITERATIONS}"
    max_tasks="$MAX_ITERATIONS"
  fi

  local spec
  spec="$(cat "$PROMPT_FILE")"

  local sys_prompt
  sys_prompt="$(load_system_prompt bootstrap)"
  sys_prompt="${sys_prompt//\{max_tasks\}/$max_tasks}"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are bootstrapping an iterative development process.

Read the following spec and produce two files:

1. acceptance.md — A markdown checklist of acceptance criteria (definition of done).
   Each item should be a "- [ ]" checkbox. These are high-level criteria that define success.

2. tasks.md — A markdown checklist of atomic implementation tasks.
   Each item should be a "- [ ]" checkbox. These are concrete, ordered steps to implement the spec.
   Each task should be small enough to complete in a single iteration.
   Order them by dependency (do foundational tasks first).
   Generate NO MORE THAN ${max_tasks} tasks.

Write both files to: ${RUN_DIR}/

Here is the spec:

${spec}
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "Bootstrap failed"
    echo "$output" > "${RUN_DIR}/iterations/0-bootstrap.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/0-bootstrap.md"

  # Validate bootstrap created required files
  if [[ ! -f "${RUN_DIR}/acceptance.md" ]]; then
    log_fail "Bootstrap failed: acceptance.md not created"
    return 1
  fi
  if [[ ! -f "${RUN_DIR}/tasks.md" ]]; then
    log_fail "Bootstrap failed: tasks.md not created"
    return 1
  fi

  # Enforce max tasks — truncate excess tasks from the checklist
  local task_count
  task_count="$(grep -c '^\- \[ \]' "${RUN_DIR}/tasks.md" 2>/dev/null || true)"
  if [[ "$task_count" -gt "$max_tasks" ]]; then
    log_warn "enforce_max_tasks: Bootstrap generated ${task_count} tasks, truncating to ${max_tasks}"
    local cut_line
    cut_line="$(grep -n '^\- \[ \]' "${RUN_DIR}/tasks.md" | sed -n "$((max_tasks + 1))p" | cut -d: -f1)"
    if [[ -n "$cut_line" ]]; then
      head -n "$((cut_line - 1))" "${RUN_DIR}/tasks.md" > "${RUN_DIR}/tasks.md.tmp"
      mv "${RUN_DIR}/tasks.md.tmp" "${RUN_DIR}/tasks.md"
    fi
  fi

  log_pass "[0/${MAX_ITERATIONS}] Bootstrap complete — $(grep -c '^\- \[ \]' "${RUN_DIR}/tasks.md" || true) tasks"
  maybe_write_state "bootstrap"
}

rebootstrap() {
  local reviews_json="$1"
  log "[re-bootstrap] Revising plan based on review feedback"

  maybe_write_state "replanning"

  # Calculate max_tasks
  local max_tasks="${MAX_TASKS}"
  if [[ -z "$max_tasks" ]]; then
    max_tasks="$MAX_ITERATIONS"
  fi
  if [[ "$max_tasks" -gt "$MAX_ITERATIONS" ]]; then
    max_tasks="$MAX_ITERATIONS"
  fi

  local spec
  spec="$(cat "$PROMPT_FILE")"

  local current_tasks=""
  if [[ -f "${RUN_DIR}/tasks.md" ]]; then
    current_tasks="$(cat "${RUN_DIR}/tasks.md")"
  fi
  local current_acceptance=""
  if [[ -f "${RUN_DIR}/acceptance.md" ]]; then
    current_acceptance="$(cat "${RUN_DIR}/acceptance.md")"
  fi

  local sys_prompt
  sys_prompt="$(load_system_prompt bootstrap)"
  sys_prompt="${sys_prompt//\{max_tasks\}/$max_tasks}"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are re-bootstrapping an iterative development process based on review feedback.

The reviewer has requested changes to the plan. Revise the tasks and acceptance criteria
based on their feedback while preserving the original spec intent.

## Original spec:
${spec}

## Current tasks.md:
${current_tasks}

## Current acceptance.md:
${current_acceptance}

## Review feedback (JSON):
${reviews_json}

## Instructions:
1. Read the review comments carefully
2. Revise tasks.md and acceptance.md based on the feedback
3. Write both files to: ${RUN_DIR}/
4. All tasks should be "- [ ]" checkboxes (unchecked)
5. Generate NO MORE THAN ${max_tasks} tasks
6. Preserve any aspects of the plan that weren't criticized
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "Re-bootstrap failed"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/0-rebootstrap.md"

  # Validate files were created
  if [[ ! -f "${RUN_DIR}/acceptance.md" || ! -f "${RUN_DIR}/tasks.md" ]]; then
    log_fail "Re-bootstrap failed: missing output files"
    return 1
  fi

  # Enforce max tasks
  local task_count
  task_count="$(grep -c '^\- \[ \]' "${RUN_DIR}/tasks.md" 2>/dev/null || true)"
  if [[ "$task_count" -gt "$max_tasks" ]]; then
    log_warn "enforce_max_tasks: Re-bootstrap generated ${task_count} tasks, truncating to ${max_tasks}"
    local cut_line
    cut_line="$(grep -n '^\- \[ \]' "${RUN_DIR}/tasks.md" | sed -n "$((max_tasks + 1))p" | cut -d: -f1)"
    if [[ -n "$cut_line" ]]; then
      head -n "$((cut_line - 1))" "${RUN_DIR}/tasks.md" > "${RUN_DIR}/tasks.md.tmp"
      mv "${RUN_DIR}/tasks.md.tmp" "${RUN_DIR}/tasks.md"
    fi
  fi

  # Clean up reviews file after processing
  rm -f "${RUN_DIR}/reviews.json"

  log_pass "[re-bootstrap] Revised plan — $(grep -c '^\- \[ \]' "${RUN_DIR}/tasks.md" || true) tasks"
  maybe_write_state "bootstrap"
}

run_oracle_phase() {
  log "[Oracle] Test-First Architect phase — building Test Guide"

  maybe_write_state "oracle"

  local spec
  spec="$(cat "$PROMPT_FILE")"

  local sys_prompt
  sys_prompt="$(load_system_prompt oracle)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are the Oracle — a Test-First Architect agent.

Your job is to help the user write a comprehensive Test Guide that defines
"correctness" BEFORE implementation begins.

## Spec:
${spec}

## Instructions:
1. Read the spec carefully
2. Guide the user through the 5 Oracle levels (Syntax, I/O, Property, Formal, Semantic)
3. Ask specific questions to build testable criteria
4. Document everything in ${RUN_DIR}/test-guide.md
5. Get user approval before completing

Use the AskUserQuestion tool to interact with the user.
Do NOT proceed to the next level until the current level has sufficient criteria.
Do NOT accept vague answers — every criterion must be specific and testable.

The Test Guide will be used during task iterations to provide context about
what "correct" means for each implementation task.
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "Oracle phase failed"
    echo "$output" > "${RUN_DIR}/iterations/oracle.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/oracle.md"

  # Verify test-guide.md was created
  if [[ ! -f "${RUN_DIR}/test-guide.md" ]]; then
    log_warn "Oracle phase completed but test-guide.md not found"
    return 1
  fi

  log_pass "[Oracle] Test Guide created at ${RUN_DIR}/test-guide.md"
  maybe_write_state "bootstrap"
}

count_remaining() {
  if [[ -f "${RUN_DIR}/tasks.md" ]]; then
    grep -c '^\- \[ \]' "${RUN_DIR}/tasks.md" || true
  else
    echo 0
  fi
}

get_next_task_name() {
  grep '^\- \[ \]' "${RUN_DIR}/tasks.md" | head -1 | sed 's/^- \[ \] //'
}

run_task_iteration() {
  local iter="$1"
  local remaining
  remaining="$(count_remaining)"
  local task_name
  task_name="$(get_next_task_name)"

  # Include previous verify feedback (informational, not blocking)
  local verify_info=""
  if [[ -f "${RUN_DIR}/iterations/$((iter - 1))-verify.md" ]]; then
    local prev_verify
    prev_verify="$(cat "${RUN_DIR}/iterations/$((iter - 1))-verify.md")"
    if echo "$prev_verify" | grep -qi "FAIL"; then
      verify_info="Note: The previous iteration's verification had failures (this is expected during incremental development — not all features are implemented yet). If any failures are relevant to YOUR task, address them:

${prev_verify}"
    fi
  fi

  # Include Test Guide if available (Oracle phase output)
  local test_guide_info=""
  if [[ -f "${RUN_DIR}/test-guide.md" ]]; then
    test_guide_info="## Test Guide (Oracle output — defines correctness criteria):
$(cat "${RUN_DIR}/test-guide.md")"
  fi

  log "[${iter}/${MAX_ITERATIONS}] Task: \"${task_name}\" — ${remaining} tasks remaining"

  maybe_write_state "task" "$task_name"

  local tasks
  tasks="$(cat "${RUN_DIR}/tasks.md")"
  local acceptance
  acceptance="$(cat "${RUN_DIR}/acceptance.md")"
  local prev_summary=""
  if [[ -f "${RUN_DIR}/iterations/$((iter - 1)).md" ]]; then
    prev_summary="Previous iteration summary:
$(cat "${RUN_DIR}/iterations/$((iter - 1)).md")"
  fi

  local sys_prompt
  sys_prompt="$(load_system_prompt task)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are on iteration ${iter} of ${MAX_ITERATIONS} in an iterative development process.

## Current tasks (${remaining} remaining):
${tasks}

## Acceptance criteria:
${acceptance}

${prev_summary}

${verify_info}

${test_guide_info}

## Instructions:
1. Pick the NEXT unchecked task ("- [ ]") from tasks.md
2. Implement it fully
3. Mark it as done by changing "- [ ]" to "- [x]" in ${RUN_DIR}/tasks.md
4. Write a brief summary of what you did to stdout

Do ONE task only. Be thorough but focused.
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "[${iter}/${MAX_ITERATIONS}] claude failed during task iteration"
    echo "claude failed with error" > "${RUN_DIR}/iterations/${iter}.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/${iter}.md"
}

run_verify() {
  local iter="$1"

  if [[ -z "$VERIFY_FILE" ]]; then
    log_pass "[${iter}/${MAX_ITERATIONS}] Verify: SKIP (no verify file)"
    echo "SKIP — no verification plan provided" > "${RUN_DIR}/iterations/${iter}-verify.md"
    maybe_write_state "verify"
    return 0
  fi

  local verify_ext="${VERIFY_FILE##*.}"
  local result
  local rc=0

  if [[ "$verify_ext" == "sh" ]]; then
    # Run shell script verification
    # Run in separate process group (setsid) so verify scripts that
    # do "kill 0" in traps don't kill the more-loop process
    result="$(setsid bash "$VERIFY_FILE" 2>&1)" || rc=$?

    if [[ $rc -eq 0 ]]; then
      log_pass "[${iter}/${MAX_ITERATIONS}] Verify: PASS ✓"
      echo "PASS" > "${RUN_DIR}/iterations/${iter}-verify.md"
      echo "$result" >> "${RUN_DIR}/iterations/${iter}-verify.md"
      if [[ "$VERBOSE" == true ]] && [[ -n "$result" ]]; then
        echo -e "${GREEN}━━━ VERIFY OUTPUT ━━━${NC}" >&2
        echo "$result" >&2
        echo -e "${GREEN}━━━ END ━━━${NC}" >&2
      fi
      maybe_write_state "verify"
      return 0
    else
      log_fail "[${iter}/${MAX_ITERATIONS}] Verify: FAIL ✗"
      echo "FAIL (exit code ${rc})" > "${RUN_DIR}/iterations/${iter}-verify.md"
      echo "$result" >> "${RUN_DIR}/iterations/${iter}-verify.md"
      if [[ "$VERBOSE" == true ]] && [[ -n "$result" ]]; then
        echo -e "${RED}━━━ VERIFY OUTPUT ━━━${NC}" >&2
        echo "$result" >&2
        echo -e "${RED}━━━ END ━━━${NC}" >&2
      fi
      maybe_write_state "verify"
      return 1
    fi
  elif [[ "$verify_ext" == "md" ]]; then
    # Use claude to evaluate markdown checklist
    local verify_content
    verify_content="$(cat "$VERIFY_FILE")"
    local iter_summary
    iter_summary="$(cat "${RUN_DIR}/iterations/${iter}.md")"
    local tasks_content
    tasks_content="$(cat "${RUN_DIR}/tasks.md")"

    local prompt
    IFS= read -r -d "" prompt <<EOF || true
You are a verification agent in an ITERATIVE development process.

## Full verification checklist (for reference):
${verify_content}

## Current task list (checked = done, unchecked = not yet):
${tasks_content}

## Latest iteration summary:
${iter_summary}

## Instructions:
This is an iterative build — NOT all tasks are complete yet.

1. Identify which task was just completed in this iteration
2. Only evaluate checklist items RELEVANT to that specific task
3. Ignore checklist items for features not yet implemented — those are for future iterations

Output exactly one of:
- PASS — if the work done in THIS iteration is correct and doesn't break anything
- FAIL — followed by what specifically is wrong with THIS iteration's work

A task that is correctly implemented should PASS even if unrelated checklist items are still unchecked.
EOF

    if ! result="$(run_claude "$prompt")"; then
      log_fail "[${iter}/${MAX_ITERATIONS}] Verify: ERROR (claude failed)"
      echo "FAIL — claude verification failed" > "${RUN_DIR}/iterations/${iter}-verify.md"
      maybe_write_state "verify"
      return 1
    fi

    echo "$result" > "${RUN_DIR}/iterations/${iter}-verify.md"

    if echo "$result" | head -5 | grep -qi "^PASS"; then
      log_pass "[${iter}/${MAX_ITERATIONS}] Verify: PASS ✓"
      if [[ "$VERBOSE" == true ]]; then
        echo -e "${GREEN}━━━ VERIFY DETAIL ━━━${NC}" >&2
        echo "$result" >&2
        echo -e "${GREEN}━━━ END ━━━${NC}" >&2
      fi
      maybe_write_state "verify"
      return 0
    else
      log_fail "[${iter}/${MAX_ITERATIONS}] Verify: FAIL ✗"
      if [[ "$VERBOSE" == true ]]; then
        echo -e "${RED}━━━ VERIFY DETAIL ━━━${NC}" >&2
        echo "$result" >&2
        echo -e "${RED}━━━ END ━━━${NC}" >&2
      fi
      maybe_write_state "verify"
      return 1
    fi
  else
    log_warn "[${iter}/${MAX_ITERATIONS}] Verify: SKIP (unsupported verify file type: .${verify_ext})"
    echo "SKIP — unsupported verify file type" > "${RUN_DIR}/iterations/${iter}-verify.md"
    maybe_write_state "verify"
    return 0
  fi
}

revert_last_task() {
  # Find the last checked-off task and uncheck it
  # Uses sed to revert only the LAST [x] match in the file
  local tmpfile
  tmpfile="$(mktemp)"

  # Find the line number of the last [x] task
  local last_checked
  last_checked="$(grep -n '^\- \[x\]' "${RUN_DIR}/tasks.md" | tail -1 | cut -d: -f1)"

  if [[ -n "$last_checked" ]]; then
    sed "${last_checked}s/^- \[x\]/- [ ]/" "${RUN_DIR}/tasks.md" > "$tmpfile"
    mv "$tmpfile" "${RUN_DIR}/tasks.md"
  else
    rm -f "$tmpfile"
  fi
}

enforce_single_task() {
  local snapshot_file="$1"
  local tasks_file="${RUN_DIR}/tasks.md"
  local max_per_iter=3  # allow small batches of related tasks

  local before_count after_count
  before_count="$(grep -c '^\- \[x\]' "$snapshot_file" 2>/dev/null || true)"
  after_count="$(grep -c '^\- \[x\]' "$tasks_file" 2>/dev/null || true)"
  local delta=$(( after_count - before_count ))

  if [[ "$delta" -le "$max_per_iter" ]]; then
    if [[ "$delta" -gt 1 ]]; then
      log_warn "enforce_single_task: Claude checked ${delta} tasks (allowed, max=${max_per_iter})"
    fi
    return 0
  fi

  log_warn "enforce_single_task: Claude checked ${delta} tasks, trimming to ${max_per_iter}"

  # Restore snapshot then check only the first N unchecked tasks
  local unchecked_lines
  unchecked_lines="$(grep -n '^\- \[ \]' "$snapshot_file" | head -"$max_per_iter" | cut -d: -f1)"
  cp "$snapshot_file" "$tasks_file"
  for line_num in $unchecked_lines; do
    sed "${line_num}s/^\- \[ \]/- [x]/" "$tasks_file" > "$tasks_file.tmp"
    mv "$tasks_file.tmp" "$tasks_file"
  done
}

run_honesty_check() {
  local iter="$1"
  local snapshot_file="$2"
  local tasks_file="${RUN_DIR}/tasks.md"

  # Extract newly checked tasks by diffing before/after
  local new_tasks
  new_tasks="$(diff "$snapshot_file" "$tasks_file" | grep '^>' | sed 's/^> //' | grep '^\- \[x\]' || true)"

  if [[ -z "$new_tasks" ]]; then
    # No new tasks were checked — nothing to verify
    log_warn "[${iter}/${MAX_ITERATIONS}] Honesty check: SKIP (no new tasks checked)"
    echo "SKIP — no new tasks checked" > "${RUN_DIR}/iterations/${iter}-honesty.md"
    return 0
  fi

  log "[${iter}/${MAX_ITERATIONS}] Honesty check — verifying task completion"

  maybe_write_state "honesty_check"

  # Get iteration summary
  local iter_summary=""
  if [[ -f "${RUN_DIR}/iterations/${iter}.md" ]]; then
    iter_summary="$(cat "${RUN_DIR}/iterations/${iter}.md")"
  fi

  local sys_prompt
  sys_prompt="$(load_system_prompt honesty)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are a honesty verification agent. Another Agent claims to have completed tasks.
Assume the claims are FALSE until you find evidence proving them true.

## Tasks checked off in this iteration:
${new_tasks}

## Agent's work summary:
${iter_summary}

## Instructions:
1. For each checked task above, search the codebase and READ the actual code
2. Verify that a substantive implementation matching the task description exists
3. Output HONEST or DISHONEST as the FIRST line, followed by per-task justification

Be skeptical. Read actual files. Do not trust the summary above.
EOF

  local result
  if ! result="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "[${iter}/${MAX_ITERATIONS}] Honesty check: ERROR (claude failed)"
    echo "DISHONEST — honesty check claude process failed" > "${RUN_DIR}/iterations/${iter}-honesty.md"
    return 1
  fi

  echo "$result" > "${RUN_DIR}/iterations/${iter}-honesty.md"

  if echo "$result" | grep -qiE '^HONEST($|[^A-Z])'; then
    log_pass "[${iter}/${MAX_ITERATIONS}] Honesty check: HONEST"
    if [[ "$VERBOSE" == true ]]; then
      echo -e "${GREEN}━━━ HONESTY DETAIL ━━━${NC}" >&2
      echo "$result" >&2
      echo -e "${GREEN}━━━ END ━━━${NC}" >&2
    fi
    return 0
  else
    log_fail "[${iter}/${MAX_ITERATIONS}] Honesty check: DISHONEST"
    if [[ "$VERBOSE" == true ]]; then
      echo -e "${RED}━━━ HONESTY DETAIL ━━━${NC}" >&2
      echo "$result" >&2
      echo -e "${RED}━━━ END ━━━${NC}" >&2
    fi
    return 1
  fi
}

run_audit_iteration() {
  local iter="$1"

  log "[${iter}/${MAX_ITERATIONS}] Audit — reviewing implementation quality"

  maybe_write_state "audit"

  local tasks
  tasks="$(cat "${RUN_DIR}/tasks.md")"
  local acceptance
  acceptance="$(cat "${RUN_DIR}/acceptance.md")"

  # Gather all verify outputs for quality signal
  local verify_history=""
  for vf in "${RUN_DIR}"/iterations/*-verify.md; do
    [[ -f "$vf" ]] || continue
    verify_history="${verify_history}
--- $(basename "$vf") ---
$(cat "$vf")
"
  done

  local sys_prompt
  sys_prompt="$(load_system_prompt audit)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are a code auditor. All tasks have been marked complete. Your job is to verify
the ACTUAL quality of the implementation — not trust that "checked off" means "done well."

## Completed tasks:
${tasks}

## Acceptance criteria:
${acceptance}

## Verification history (all prior verify outputs):
${verify_history}

## Instructions:
1. Read the ACTUAL implementation code for EVERY completed task
2. For each task, assess honestly:
   - SOLID — correctly implemented, handles edge cases, clean code
   - WEAK — works but has issues (missing edge cases, fragile logic, poor structure)
   - INCOMPLETE — marked done but not fully implemented, or implementation doesn't match the task description
3. List specific issues found (file path, line number, concrete problem)
4. Write your audit to ${RUN_DIR}/audit.md in this format:

## Audit Summary
<2-3 sentences on overall implementation quality>

## Task Ratings

| Task | Rating | Issues |
|------|--------|--------|
| <task name> | SOLID/WEAK/INCOMPLETE | <specific issue or "—"> |

## Priority Fixes
1. <most critical issue — file:line — what's wrong>
2. <second issue>
3. <third issue>

Do NOT fix anything. Only audit and report. Be brutally honest — the purpose of this
audit is to guide subsequent improvement iterations. Sugarcoating wastes everyone's time.
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "[${iter}/${MAX_ITERATIONS}] claude failed during audit iteration"
    echo "claude failed with error" > "${RUN_DIR}/iterations/${iter}.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/${iter}.md"
  log_pass "[${iter}/${MAX_ITERATIONS}] Audit complete — results in audit.md"
}

run_improve_iteration() {
  local iter="$1"

  log "[${iter}/${MAX_ITERATIONS}] Improve mode — all tasks complete"

  local tasks
  tasks="$(cat "${RUN_DIR}/tasks.md")"
  local acceptance
  acceptance="$(cat "${RUN_DIR}/acceptance.md")"

  # Build audit context
  local audit_findings=""
  if [[ -f "${RUN_DIR}/audit.md" ]]; then
    audit_findings="## Audit findings (address WEAK/INCOMPLETE items by priority):
$(cat "${RUN_DIR}/audit.md")"
  fi

  # Build improve history (prevent repeats)
  local improve_history=""
  if [[ -f "${RUN_DIR}/improve-log.md" ]]; then
    # Limit to last 100 lines to avoid prompt bloat
    improve_history="## Previous improvements already made (DO NOT repeat these):
$(tail -100 "${RUN_DIR}/improve-log.md")"
  fi

  # Include latest verify output as quality signal
  local verify_detail=""
  if [[ -f "${RUN_DIR}/iterations/${iter}-verify.md" ]]; then
    verify_detail="## Latest verification output:
$(cat "${RUN_DIR}/iterations/${iter}-verify.md")"
  fi

  local sys_prompt
  sys_prompt="$(load_system_prompt improve)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are in improvement mode. All planned tasks are complete and verification passes.

## Completed tasks:
${tasks}

## Acceptance criteria:
${acceptance}

${audit_findings}

${verify_detail}

${improve_history}

## Instructions:
PHASE 1 — AUDIT (do this first, report findings):
- Read the actual implementation code for the areas flagged in the audit findings above
- If no audit findings, read the implementation for each completed task
- Identify the weakest area: incomplete implementations, fragile logic,
  missing edge cases, untested paths, unclear code

PHASE 2 — ACT (one improvement only):
- Pick the single highest-impact finding from your audit
- Implement the fix/improvement completely
- If a previous improvement (listed above) already addressed something, skip it

Write your output as:
AUDIT: <2-3 sentence summary of what you found>
ACTION: <what you implemented and why>
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "[${iter}/${MAX_ITERATIONS}] claude failed during improve iteration"
    echo "claude failed with error" > "${RUN_DIR}/iterations/${iter}.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/${iter}.md"

  # Append to improve log for cumulative context across iterations
  {
    echo "## Iteration ${iter}"
    echo "$output"
    echo ""
  } >> "${RUN_DIR}/improve-log.md"
}

run_task_fix_iteration() {
  local iter="$1"

  log "[${iter}/${MAX_ITERATIONS}] Fix mode — verify failed, addressing issues"

  local tasks
  tasks="$(cat "${RUN_DIR}/tasks.md")"
  local acceptance
  acceptance="$(cat "${RUN_DIR}/acceptance.md")"

  # Get the most recent verify failure details
  # The verify for this iteration was just run (same iter number)
  local verify_feedback=""
  local cur_verify="${RUN_DIR}/iterations/${iter}-verify.md"
  if [[ -f "$cur_verify" ]]; then
    verify_feedback="$(cat "$cur_verify")"
  fi

  maybe_write_state "task" "fix: verify failures"

  local sys_prompt
  sys_prompt="$(load_system_prompt task)"

  local prompt
  IFS= read -r -d "" prompt <<EOF || true
You are in FIX mode. All tasks are marked complete but verification is failing.

## Completed tasks:
${tasks}

## Acceptance criteria:
${acceptance}

## Verification failure details:
${verify_feedback}

## Instructions:
1. Read the verification failure output carefully
2. Fix ONLY the issues that are causing verification to fail
3. Do NOT modify tasks.md — all tasks are already complete
4. Write a brief summary of what you fixed to stdout

Focus on making the verify pass. Do not refactor or improve unrelated code.
EOF

  local output
  if ! output="$(run_claude "$prompt" "$sys_prompt")"; then
    log_fail "[${iter}/${MAX_ITERATIONS}] claude failed during fix iteration"
    echo "claude failed with error" > "${RUN_DIR}/iterations/${iter}.md"
    return 1
  fi

  echo "$output" > "${RUN_DIR}/iterations/${iter}.md"
}

# Web server management
SERVER_PID=""

start_web_server() {
  local server_path="${DATA_DIR}/server.py"
  local dashboard_path="${DATA_DIR}/dashboard.html"

  # Check if dashboard files exist
  if [[ ! -f "$server_path" ]]; then
    log_fail "Web dashboard not installed. Run install.sh to enable."
    exit 1
  fi

  if [[ ! -f "$dashboard_path" ]]; then
    log_fail "Web dashboard not installed. Run install.sh to enable."
    exit 1
  fi

  # Build command args
  local server_args=("${RUN_DIR}")
  [[ -n "$WEB_PORT" ]] && server_args+=("$WEB_PORT")

  # Start server in background
  local server_stderr
  server_stderr="$(mktemp)"
  python3 "$server_path" "${server_args[@]}" 2>"$server_stderr" &
  SERVER_PID=$!

  # Poll for .server.url file written by server.py (up to 10 seconds)
  local url_file="${RUN_DIR}/.server.url"
  local dashboard_url=""
  local waited=0
  while [[ $waited -lt 100 ]]; do
    if ! kill -0 "$SERVER_PID" 2>/dev/null; then
      log_fail "Error: Failed to start web server"
      cat "$server_stderr" >&2
      rm -f "$server_stderr"
      exit 1
    fi
    if [[ -s "$url_file" ]]; then
      dashboard_url="$(cat "$url_file")"
      break
    fi
    sleep 0.1
    waited=$((waited + 1))
  done
  rm -f "$server_stderr"

  if [[ -z "$dashboard_url" ]]; then
    log_fail "Error: Web server started but failed to report its URL"
    exit 1
  fi

  # Clean up web server on exit; on signals, also terminate the script
  trap "stop_web_server" EXIT
  trap "stop_web_server; exit 130" SIGINT
  trap "stop_web_server; exit 143" SIGTERM

  # Save PID for cleanup
  echo "$SERVER_PID" > "${RUN_DIR}/.server.pid"

  # Print dashboard URL
  echo "" >&2
  echo "============================================" >&2
  echo "  Web Dashboard running at:" >&2
  echo "  ${dashboard_url}" >&2
  echo "============================================" >&2
  echo "" >&2
}

stop_web_server() {
  if [[ -n "$SERVER_PID" ]] && kill -0 "$SERVER_PID" 2>/dev/null; then
    kill "$SERVER_PID" 2>/dev/null || true
    wait "$SERVER_PID" 2>/dev/null || true
  fi

  # Also try to kill using saved PID file
  if [[ -f "${RUN_DIR}/.server.pid" ]]; then
    local saved_pid
    saved_pid="$(cat "${RUN_DIR}/.server.pid")"
    if [[ -n "$saved_pid" ]] && kill -0 "$saved_pid" 2>/dev/null; then
      kill "$saved_pid" 2>/dev/null || true
    fi
    rm -f "${RUN_DIR}/.server.pid"
  fi
  rm -f "${RUN_DIR}/.server.url"
}

check_stop_signal() {
  # Returns 0 (true) if stop signal file exists
  [[ -f "${RUN_DIR}/.signal-stop" ]]
}

handle_request_changes() {
  local request_changes_file="${RUN_DIR}/.signal-request-changes"
  local reviews_file="${RUN_DIR}/reviews.json"

  rm -f "$request_changes_file"
  log ""
  log_warn "Review changes requested, re-planning..."

  # Read reviews
  local reviews_json="[]"
  if [[ -f "$reviews_file" ]]; then
    reviews_json="$(cat "$reviews_file")"
  fi

  # Re-bootstrap with review feedback
  if rebootstrap "$reviews_json"; then
    log_pass "Plan revised successfully"
  else
    log_fail "Re-bootstrap failed, keeping original plan"
  fi
}

wait_for_approval() {
  local context="$1"  # "bootstrap" or "iteration"

  # Update state to waiting_approval
  maybe_write_state "waiting_approval"

  local approve_file="${RUN_DIR}/.signal-approve"
  local request_changes_file="${RUN_DIR}/.signal-request-changes"
  local timeout="${APPROVE_TIMEOUT}"

  # Handle infinite timeout (0)
  if [[ "$timeout" -eq 0 ]]; then
    timeout=999999999
  fi

  log ""
  log "⏸ Approval required: ${context}"
  log ""

  if [[ "$WEB_MODE" == "true" ]]; then
    # Web mode: poll for signal file
    local elapsed=0
    local poll_interval=1

    log "Waiting for approval via web dashboard..."
    log "(Press Ctrl+C to stop)"

    while [[ $elapsed -lt $timeout ]]; do
      # Check for stop signal
      if check_stop_signal; then
        log ""
        log_warn "Stop signal received, exiting..."
        rm -f "${RUN_DIR}/.signal-stop"
        return 1
      fi

      # Check for approve signal
      if [[ -f "$approve_file" ]]; then
        rm -f "$approve_file"
        log_pass "Approved via dashboard, continuing..."
        log ""
        return 0
      fi

      # Check for request-changes signal
      if [[ -f "$request_changes_file" ]]; then
        handle_request_changes
        # Reset timeout and continue waiting for approval
        elapsed=0
        maybe_write_state "waiting_approval"
        log ""
        log "Waiting for approval of revised plan..."
        continue
      fi

      # Restart web server if it died
      if [[ -n "${SERVER_PID:-}" ]] && ! kill -0 "$SERVER_PID" 2>/dev/null; then
        log_warn "Web server died, restarting..."
        start_web_server
      fi

      sleep "$poll_interval"
      elapsed=$((elapsed + poll_interval))
    done

    # Timeout reached
    log_warn "Approval timeout (${APPROVE_TIMEOUT}s), auto-continuing..."
    log ""
    return 0

  else
    # Terminal mode: prompt with countdown
    local remaining=$timeout

    echo "" >&2
    echo -e "${BLUE}${BOLD}⏸ Approval required: ${context}${NC}" >&2
    echo "" >&2
    echo "Press Enter to continue, or Ctrl+C to stop..." >&2

    # Countdown timer with live display
    while [[ $remaining -gt 0 ]]; do
      # Show countdown on same line (using carriage return)
      echo -ne "Auto-continue in ${remaining}s...\r" >&2

      # Check for stop signal (if somehow created)
      if check_stop_signal; then
        echo "" >&2
        log_warn "Stop signal received, exiting..."
        rm -f "${RUN_DIR}/.signal-stop"
        return 1
      fi

      # Check for request-changes signal
      if [[ -f "$request_changes_file" ]]; then
        echo "" >&2
        handle_request_changes
        # Reset timeout and continue waiting
        remaining=$timeout
        maybe_write_state "waiting_approval"
        echo "" >&2
        echo "Press Enter to continue, or Ctrl+C to stop..." >&2
        continue
      fi

      # Read with timeout (1 second increments)
      if read -r -t 1 2>/dev/null; then
        # User pressed Enter
        echo "" >&2
        log_pass "Approved via terminal, continuing..."
        echo "" >&2
        return 0
      fi

      remaining=$((remaining - 1))
    done

    # Timeout reached - clear the countdown line
    echo -ne "\r$(printf '%*s' 30 '')\r" >&2
    log_warn "Approval timeout (${APPROVE_TIMEOUT}s), auto-continuing..."
    echo "" >&2
    return 0
  fi
}

main() {
  parse_args "$@"

  local start_iter=1

  if [[ -n "$RESUME_DIR" ]]; then
    resume_run_dir
    start_iter="$RESUME_FROM"
    # -n means additional iterations when resuming
    MAX_ITERATIONS=$(( start_iter + MAX_ITERATIONS - 1 ))
    local remaining
    remaining="$(count_remaining)"
    log "Resuming more-loop: iterations ${start_iter}–${MAX_ITERATIONS}, model=${MODEL}"
    log "Run directory: ${RUN_DIR}"
    log "Resuming from iteration ${start_iter} — ${remaining} tasks remaining"
  else
    setup_run_dir
    log "Starting more-loop: ${MAX_ITERATIONS} iterations, model=${MODEL}"
    log "Run directory: ${RUN_DIR}"
  fi
  echo "" >&2

  # Start web server if --web flag is set
  if [[ "$WEB_MODE" == "true" ]]; then
    start_web_server
  fi

  # Write initial state before bootstrap
  maybe_write_state "bootstrap"

  # Bootstrap phase (skip on resume)
  if [[ -z "$RESUME_DIR" ]]; then
    if ! bootstrap; then
      log_fail "Bootstrap failed, aborting"
      exit 1
    fi
    echo "" >&2

    # Approval checkpoint after bootstrap
    if [[ "$APPROVE_MODE" == "true" ]]; then
      if ! wait_for_approval "bootstrap"; then
        log "Run stopped by user"
        maybe_write_state "done"
        exit 0
      fi
    fi
  fi

  # Oracle phase (if enabled)
  if [[ "$ORACLE_MODE" == "true" ]]; then
    if ! run_oracle_phase; then
      log_warn "Oracle phase failed, continuing without Test Guide"
    fi
    echo ""

    # Approval checkpoint after Oracle
    if [[ "$APPROVE_MODE" == "true" ]]; then
      if ! wait_for_approval "oracle"; then
        log "Run stopped by user"
        maybe_write_state "done"
        exit 0
      fi
    fi
  fi

  # Main loop
  local iter="$start_iter"
  while [[ $iter -le $MAX_ITERATIONS ]]; do
    # Check for stop signal at start of each iteration
    if check_stop_signal; then
      log_warn "Stop signal received, exiting..."
      maybe_write_state "done"
      rm -f "${RUN_DIR}/.signal-stop"
      exit 0
    fi

    CURRENT_ITERATION="$iter"
    local remaining
    remaining="$(count_remaining)"

    # Write state at start of each iteration
    maybe_write_state "iteration_start"

    if [[ "$remaining" -gt 0 ]]; then
      # Task mode — snapshot for enforce_single_task and honesty check
      cp "${RUN_DIR}/tasks.md" "${RUN_DIR}/.tasks-snapshot.md"
      run_task_iteration "$iter" || true
      enforce_single_task "${RUN_DIR}/.tasks-snapshot.md"

      # Honesty check — verify agent actually implemented the task
      # If dishonest, revert task and skip verify
      if run_honesty_check "$iter" "${RUN_DIR}/.tasks-snapshot.md"; then
        rm -f "${RUN_DIR}/.tasks-snapshot.md"

        # Verify — informational only, no rollback
        # Results are logged and fed to next iteration as feedback
        run_verify "$iter" || true
      else
        # Restore snapshot — reverts ALL newly checked tasks, not just the last one
        cp "${RUN_DIR}/.tasks-snapshot.md" "${RUN_DIR}/tasks.md"
        rm -f "${RUN_DIR}/.tasks-snapshot.md"
        log_warn "[${iter}/${MAX_ITERATIONS}] Task reverted — will retry next iteration"
      fi
    else
      # All tasks done — first run audit, then improve/fix cycle
      if [[ ! -f "${RUN_DIR}/audit.md" ]]; then
        # One-time audit: review actual implementation quality
        run_audit_iteration "$iter" || true
      else
        # Audit exists — verify is now a gate
        if run_verify "$iter"; then
          # Verify passed — enter directed improvement mode
          run_improve_iteration "$iter" || true
        else
          # Verify failed — enter fix mode (Claude sees failure feedback)
          log_warn "Verify failed after all tasks — entering fix iteration"
          run_task_fix_iteration "$iter" || true
        fi
      fi
    fi

    # Approval checkpoint after each iteration (only if --approve-every)
    if [[ "$APPROVE_EVERY" == "true" ]]; then
      if ! wait_for_approval "iteration ${iter}"; then
        log "Run stopped by user"
        maybe_write_state "done"
        exit 0
      fi
    fi

    echo "" >&2
    iter=$((iter + 1))
  done

  remaining="$(count_remaining)"
  echo "" >&2
  if [[ "$remaining" -eq 0 ]]; then
    log_pass "All tasks complete after ${MAX_ITERATIONS} iterations"
  else
    log_warn "${remaining} tasks remaining after ${MAX_ITERATIONS} iterations"
  fi
  log "Results in: ${RUN_DIR}/"
  maybe_write_state "done"
}

# Features: Review Comment|Request change|re-bootstrap|rebootstrap|re_bootstrap
# Config: approve_timeout|APPROVE_TIMEOUT passed to write_state_json

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  main "$@"
fi
